{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f0cafc0-1993-411c-9247-d75ac091280b",
   "metadata": {},
   "source": [
    "# Performance Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67ce4a31-1781-4f46-a7ce-e2b1f6cedc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost     : 1.5.1\n",
      "pandas      : 1.3.4\n",
      "numpy       : 1.21.3\n",
      "scikit-learn: 0.0\n",
      "\n",
      "Python 3.8.12\n"
     ]
    }
   ],
   "source": [
    "import watermark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import copy\n",
    "import os\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import ray\n",
    "from tqdm import tqdm\n",
    "\n",
    "%reload_ext watermark\n",
    "%watermark -p xgboost,pandas,numpy,scikit-learn\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381c55c2-1ec2-43fc-8c66-4a2acbc4b857",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf2e92fa-1bf4-4435-a1f3-4e9613ec83d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        userId  movieId  rating   timestamp\n",
      "0            1        1     4.0   964982703\n",
      "1            1        3     4.0   964981247\n",
      "2            1        6     4.0   964982224\n",
      "3            1       47     5.0   964983815\n",
      "4            1       50     5.0   964982931\n",
      "...        ...      ...     ...         ...\n",
      "100831     610   166534     4.0  1493848402\n",
      "100832     610   168248     5.0  1493850091\n",
      "100833     610   168250     5.0  1494273047\n",
      "100834     610   168252     5.0  1493846352\n",
      "100835     610   170875     3.0  1493846415\n",
      "\n",
      "[100836 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read ratings df\n",
    "path = \"data_small\"\n",
    "use_large = False\n",
    "if (use_large):\n",
    "    path = \"data_large\"\n",
    "    ratings = pd.read_csv('./' + path + '/ratings.csv')[:1000000]\n",
    "else:\n",
    "    ratings = pd.read_csv('./' + path + '/ratings.csv')\n",
    "\n",
    "movies = pd.read_csv('./' + path + '/movies.csv')\n",
    "\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fefb6be-d101-4bac-ab62-fe647d42e9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9737</th>\n",
       "      <td>193581</td>\n",
       "      <td>Black Butler: Book of the Atlantic (2017)</td>\n",
       "      <td>Action|Animation|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9738</th>\n",
       "      <td>193583</td>\n",
       "      <td>No Game No Life: Zero (2017)</td>\n",
       "      <td>Animation|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>193585</td>\n",
       "      <td>Flint (2017)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9740</th>\n",
       "      <td>193587</td>\n",
       "      <td>Bungo Stray Dogs: Dead Apple (2018)</td>\n",
       "      <td>Action|Animation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9741</th>\n",
       "      <td>193609</td>\n",
       "      <td>Andrew Dice Clay: Dice Rules (1991)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9742 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      movieId                                      title  \\\n",
       "0           1                           Toy Story (1995)   \n",
       "1           2                             Jumanji (1995)   \n",
       "2           3                    Grumpier Old Men (1995)   \n",
       "3           4                   Waiting to Exhale (1995)   \n",
       "4           5         Father of the Bride Part II (1995)   \n",
       "...       ...                                        ...   \n",
       "9737   193581  Black Butler: Book of the Atlantic (2017)   \n",
       "9738   193583               No Game No Life: Zero (2017)   \n",
       "9739   193585                               Flint (2017)   \n",
       "9740   193587        Bungo Stray Dogs: Dead Apple (2018)   \n",
       "9741   193609        Andrew Dice Clay: Dice Rules (1991)   \n",
       "\n",
       "                                           genres  \n",
       "0     Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                      Adventure|Children|Fantasy  \n",
       "2                                  Comedy|Romance  \n",
       "3                            Comedy|Drama|Romance  \n",
       "4                                          Comedy  \n",
       "...                                           ...  \n",
       "9737              Action|Animation|Comedy|Fantasy  \n",
       "9738                     Animation|Comedy|Fantasy  \n",
       "9739                                        Drama  \n",
       "9740                             Action|Animation  \n",
       "9741                                       Comedy  \n",
       "\n",
       "[9742 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c39be7f4-5f67-4d6f-b5f2-0470eb9ee31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9742/9742 [00:00<00:00, 1415047.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(no genres listed)', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'IMAX', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "genres_set = set()\n",
    "for string in tqdm(movies.genres):\n",
    "    genres_set.update(string.split('|'))\n",
    "#genres.remove('(no genres listed)')\n",
    "genres = sorted(genres_set)\n",
    "print(genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62c88906-630e-4cc3-869d-984598f09887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9742/9742 [00:20<00:00, 474.07it/s]\n"
     ]
    }
   ],
   "source": [
    "for genre in genres:\n",
    "    ratings[genre] = False\n",
    "\n",
    "for movie in tqdm(movies.movieId):\n",
    "    for genre in movies.genres[movies.loc[movies.movieId == movie].index[0]].split('|'):\n",
    "        ratings.loc[ratings.movieId == movie, [genre]] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37c3f66e-fe47-41bf-b960-2cdda6884d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100836, 24)\n",
      "(62518, 24)\n"
     ]
    }
   ],
   "source": [
    "# Delete movies with fewer than 25 ratings\n",
    "data = ratings.pivot(index = \"userId\", columns = \"movieId\", values = \"rating\")\n",
    "print(ratings.shape)\n",
    "to_del = list()\n",
    "counts = data.count(axis = 0, numeric_only = True)\n",
    "for i in data.columns.values:\n",
    "    if (counts[i] < 25):\n",
    "        to_del.append(i)\n",
    "ratings = ratings[~(ratings.movieId.isin(to_del))]\n",
    "print(ratings.shape)\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bd828e7-58b2-4e9e-83ce-19d2e8c7a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def insufficient_ratings(dataset, user):\n",
    "    user_ratings = sum(dataset.userId == user)\n",
    "    if (user_ratings < 20):\n",
    "        return user\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7222a454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 610/610 [00:01<00:00, 483.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 61 users with fewer than 20 ratings. They have been removed from the dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Removes users who no longer have 20 ratings\n",
    "\n",
    "def to_iterator(obj_ids):\n",
    "    while obj_ids:\n",
    "        done, obj_ids = ray.wait(obj_ids)\n",
    "        yield ray.get(done[0])\n",
    "\n",
    "dat = ray.put(ratings)\n",
    "obj_ids = [insufficient_ratings.remote(dat, user) for user in ratings.userId.unique()]\n",
    "results = []\n",
    "for x in tqdm(to_iterator(obj_ids), total = len(obj_ids)):\n",
    "    if x > -1:\n",
    "        results.append(x)\n",
    "\n",
    "ratings = ratings.loc[~(ratings.userId.isin(results))]\n",
    "\n",
    "del dat\n",
    "        \n",
    "print(\"There were\", len(results), \"users with fewer than 20 ratings. They have been removed from the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27ad3e85-a683-40e8-b334-8e1da4936a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Valid/Test sizes: 38700 10195 12724\n"
     ]
    }
   ],
   "source": [
    "# Note that GroupShuffleSplit allows us to have distinct users in the training, validation, and test sets\n",
    "\n",
    "# Split into 80-20 training - testing split\n",
    "train_inds, test_inds = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 42).split(ratings, groups=ratings.userId))\n",
    "\n",
    "train_val = ratings.iloc[train_inds].copy()\n",
    "test = ratings.iloc[test_inds].copy()\n",
    "\n",
    "# Split into 80-20 training - validation split\n",
    "train_inds, val_inds = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 42).split(train_val, groups=train_val.userId))\n",
    "\n",
    "training = train_val.iloc[train_inds].copy()\n",
    "validation = train_val.iloc[val_inds].copy()\n",
    "\n",
    "print('Train/Valid/Test sizes:', training.shape[0], validation.shape[0], test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec799c23-52a6-404f-927a-96b54d7964bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As it is, this will take the 10 most recent ratings for a user to use for prediction\n",
    "# Commented out sections allow random ratings to be taken\n",
    "\n",
    "@ray.remote\n",
    "def prepare_helper(user, dataset):\n",
    "    #random.seed(1)\n",
    "    user_ratings = dataset.loc[dataset.userId == user]\n",
    "                               \n",
    "    # Randomly select 10 movies to remove\n",
    "    #selected_movies = random.sample(range(0, user_ratings.shape[0]), 10)\n",
    "    #selected_movies = random.choices(list(user_ratings.movieId), k=10)\n",
    "                               \n",
    "    # This will select the 10 most recent ratings for this user\n",
    "    selected_movies = user_ratings.sort_values(by = \"timestamp\", ascending = False).movieId.values[0:10]\n",
    "    \n",
    "    # Add the removed movies to the removed dataframe\n",
    "    removed_movies = user_ratings[user_ratings.movieId.isin(selected_movies)]\n",
    "    \n",
    "    return removed_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dd65052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in either the validation or training set\n",
    "# Removes the most recent 10 ratings for each user, and returns them as removed_movies\n",
    "# Returns the original dataset with the ratings to predict removed as new_movies\n",
    "def prepare_data(dataset):\n",
    "    # For validation and test data, remove 10 rated movies to be predicted\n",
    "    removed_movies = pd.DataFrame(columns = dataset.columns)\n",
    "    # Iterate over each user\n",
    "    \n",
    "    def to_iterator(obj_ids):\n",
    "        while obj_ids:\n",
    "            done, obj_ids = ray.wait(obj_ids)\n",
    "            yield ray.get(done[0])\n",
    "    dat = ray.put(dataset)\n",
    "    obj_ids = [prepare_helper.remote(user, dat) for user in dataset[\"userId\"].unique()]\n",
    "    results = []\n",
    "    for x in tqdm(to_iterator(obj_ids), total = len(obj_ids)):\n",
    "        results.append(x)\n",
    "    \n",
    "    # Combine removed movies:\n",
    "    for result in tqdm(results):\n",
    "        removed_movies = pd.concat([removed_movies, result])\n",
    "        \n",
    "    # Remove all of the user-movie combinations in removed_moviews from dataset\n",
    "    new_movies = pd.merge(dataset, removed_movies, indicator=True, how='outer').query('_merge==\"left_only\"').drop('_merge', axis=1)\n",
    "    del dat\n",
    "    \n",
    "    return removed_movies, new_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e73b9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set already exists\n",
      "Test set already exists\n"
     ]
    }
   ],
   "source": [
    "# Get the proper splits for the validation and testing sets\n",
    "if (os.path.isfile('./' + path + '/validation_removed_tmp.csv') and os.path.isfile('./' + path + '/validation_new.csv')):\n",
    "    print(\"Validation set already exists\" )\n",
    "    validation_removed = pd.read_csv('./' + path + '/validation_removed_tmp.csv')\n",
    "    validation_new = pd.read_csv('./' + path + '/validation_new.csv')\n",
    "else:\n",
    "    start = datetime.datetime.now()\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "    validation_removed, validation_new = prepare_data(validation)\n",
    "    print('Time building validation set: {}'.format(datetime.datetime.now()-start))\n",
    "    validation_removed.to_csv('./' + path + '/validation_removed_tmp.csv')\n",
    "    validation_new.to_csv('./' + path + '/validation_new.csv')\n",
    "\n",
    "if (os.path.isfile('./' + path + '/test_removed_tmp.csv') and os.path.isfile('./' + path + '/test_new.csv')):\n",
    "    print(\"Test set already exists\" )\n",
    "    test_removed = pd.read_csv('./' + path + '/test_removed_tmp.csv')\n",
    "    test_new = pd.read_csv('./' + path + '/test_new.csv')                                                                \n",
    "else:\n",
    "    start = datetime.datetime.now()\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "    test_removed, test_new = prepare_data(test)\n",
    "    print('Time building test set: {}'.format(datetime.datetime.now()-start))\n",
    "    test_removed.to_csv('./' + path + '/test_removed_tmp.csv')\n",
    "    test_new.to_csv('./' + path + '/test_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5385e4b2-01f5-4f9d-baa9-f5e540b21c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return tuple of (user, average rating for user)\n",
    "@ray.remote\n",
    "def add_user(dataset, user):\n",
    "    return (user, np.mean(dataset[dataset[\"userId\"] == user][\"rating\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca37d3e9-b046-4f58-956f-e997561c41a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns dataset with the average user rating for each user\n",
    "# Optionally takes in removed when using validation or test data\n",
    "def user_avg(dataset, using_removed = False, removed = pd.DataFrame()):\n",
    "    def to_iterator(obj_ids):\n",
    "        while obj_ids:\n",
    "            done, obj_ids = ray.wait(obj_ids)\n",
    "            yield ray.get(done[0])\n",
    "\n",
    "    # Get the user averages\n",
    "    dataset[\"userAvg\"] = 0.0\n",
    "    dat = ray.put(dataset)\n",
    "    obj_ids = ([add_user.remote(dat, user) for user in dataset.userId.unique()])\n",
    "    results = []\n",
    "    for x in tqdm(to_iterator(obj_ids), total = len(obj_ids)):\n",
    "        results.append(x)\n",
    "    del dat\n",
    "\n",
    "    if (using_removed):\n",
    "        removed[\"userAvg\"] = 0.0\n",
    "    print(\"Combining data\")\n",
    "    for result in tqdm(results):\n",
    "        dataset.loc[dataset[\"userId\"] == result[0], [\"userAvg\"]] = result[1]\n",
    "        if (using_removed):\n",
    "            removed.loc[removed.userId == result[0], [\"userAvg\"]] = result[1]\n",
    "    \n",
    "    return dataset, removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02ddf8db-9e02-407a-8dba-338d34eec657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return tuple of (movie, average rating for movie)\n",
    "@ray.remote\n",
    "def add_movie(dataset, movie):\n",
    "    return (movie, np.mean(dataset[dataset[\"movieId\"] == movie][\"rating\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12240444-0490-44ad-87df-7b93afc2ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns dataset with the average movie rating for each movie\n",
    "def movie_avg(dataset, removed):\n",
    "    def to_iterator(obj_ids):\n",
    "        while obj_ids:\n",
    "            done, obj_ids = ray.wait(obj_ids)\n",
    "            yield ray.get(done[0])\n",
    "\n",
    "    # Get the movie averages\n",
    "    dataset[\"movieAvg\"] = 0.0\n",
    "    dat = ray.put(dataset)\n",
    "    obj_ids = ([add_movie.remote(dat, movie) for movie in dataset.movieId.unique()])\n",
    "    results = []\n",
    "    for x in tqdm(to_iterator(obj_ids), total = len(obj_ids)):\n",
    "        results.append(x)\n",
    "    del dat\n",
    "    \n",
    "    removed[\"movieAvg\"] = 0.0\n",
    "    print(\"Combining data\")\n",
    "    for result in tqdm(results):\n",
    "        dataset.loc[dataset[\"movieId\"] == result[0], [\"movieAvg\"]] = result[1]\n",
    "        removed.loc[removed.movieId == result[0], [\"movieAvg\"]] = result[1]\n",
    "        \n",
    "    return dataset, removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f67d751-3753-4522-b63c-b22a7e550785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return datasets with average movie rating and average user rating for each row\n",
    "def add_avgs(training, validation_new, test_new, validation_removed, test_removed):\n",
    "    print(\"\\nCalculating training set user averages\")\n",
    "    training, dummy = user_avg(training, False)\n",
    "    print(\"\\nCalculating validation set user averages\")\n",
    "    validation_new, validation_removed = user_avg(validation_new, True, validation_removed)\n",
    "    print(\"\\nCalculating test set user averages\")\n",
    "    test_new, test_removed = user_avg(test_new, True, test_removed)\n",
    "    \n",
    "    print(\"\\nCalculating validation set movie averages\")\n",
    "    training_validation, validation_removed = movie_avg(pd.concat([training, validation_new]), validation_removed)\n",
    "    \n",
    "    print(\"\\nCalculating test set movie averages\")\n",
    "    training_test, test_removed = movie_avg(pd.concat([training, test_new]), test_removed)\n",
    "    \n",
    "    return training_validation, training_test, validation_removed, test_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d39ff1ce-2647-43a2-a457-24f225c47744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sets already exist\n"
     ]
    }
   ],
   "source": [
    "if (os.path.isfile('./' + path + '/training_validation.csv') and os.path.isfile('./' + path + '/training_test.csv') and os.path.isfile('./' + path + '/validation_removed.csv') and os.path.isfile('./' + path + '/test_removed.csv')):\n",
    "    print(\"Sets already exist\" )\n",
    "    training_validation = pd.read_csv('./' + path + '/training_validation.csv')\n",
    "    training_test = pd.read_csv('./' + path + '/training_test.csv')\n",
    "    validation_removed = pd.read_csv('./' + path + '/validation_removed.csv')\n",
    "    test_removed = pd.read_csv('./' + path + '/test_removed.csv')\n",
    "else:\n",
    "    \n",
    "    # training_validation has all training data as well as the unremoved values in the validation data\n",
    "    # this will be used to train, and we will use the removed validation data to predict\n",
    "    \n",
    "    # training_test has all training data as well as the unremoved values in the test data\n",
    "    # this will be used to train, and we will use the removed test data to predict\n",
    "    \n",
    "    training_validation, training_test, validation_removed, test_removed = add_avgs(training, validation_new, test_new, validation_removed, test_removed)\n",
    "    training_validation.to_csv('./' + path + '/training_validation.csv')\n",
    "    training_test.to_csv('./' + path + '/training_test.csv')\n",
    "    validation_removed.to_csv('./' + path + '/validation_removed.csv')\n",
    "    test_removed.to_csv('./' + path + '/test_removed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e7c027-a164-4969-8e62-4f9d2e629575",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19408600-e639-4c11-b82a-4e60369b8c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac59bb82-103d-4bef-a865-fd949778126e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classification Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9a52056-52cc-405b-8ce9-11d16e87fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = copy.deepcopy(validation_removed['rating']) \n",
    "y_val = np.ravel(y_val)\n",
    "y_val = pd.cut(y_val, bins=[0,3,5], labels=[0,1]) # convert continuous to categorical\n",
    "\n",
    "y_test = copy.deepcopy(test_removed['rating']) \n",
    "y_test = np.ravel(y_test)\n",
    "y_test = pd.cut(y_test, bins=[0,3,5], labels=[0,1]) # convert continuous to categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266605ec-ea31-4f38-917d-00dae2d05890",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Validation Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0378a33-cf1f-44df-8e59-145710b9e6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439/439 [00:00<00:00, 1000.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Validation Accuracy for user avg: 71.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 550/550 [00:00<00:00, 1014.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Test Accuracy for movie avg: 72.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline user average\n",
    "ratings_pred = validation_removed.copy(deep=True)\n",
    "\n",
    "for user in tqdm(training_validation[\"userId\"].unique()):\n",
    "    ratings_pred.loc[ratings_pred[\"userId\"] == user, [\"rating\"]] = np.digitize(training_validation.loc[training_validation.userId == user, \"userAvg\"].iat[0], bins=[3], right = False)\n",
    "\n",
    "avg_user_accuracy = accuracy_score(y_val, ratings_pred[\"rating\"])\n",
    "print(\"Baseline Validation Accuracy for user avg: %.2f%%\" % (avg_user_accuracy * 100))\n",
    "\n",
    "# Baseline movie average\n",
    "ratings_pred = validation_removed.copy(deep=True)\n",
    "training_movies = training_validation.movieId.unique()\n",
    "\n",
    "for movie in tqdm(validation_removed.movieId.unique()):\n",
    "    if movie in training_movies:\n",
    "        ratings_pred.loc[ratings_pred.movieId == movie, [\"rating\"]] = np.digitize(training_validation.loc[training_validation.movieId == movie, \"movieAvg\"].iat[0], bins=[3], right = False)\n",
    "    \n",
    "avg_movie_accuracy = accuracy_score(y_val, ratings_pred[\"rating\"])\n",
    "print(\"Baseline Test Accuracy for movie avg: %.2f%%\" % (avg_movie_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d8b782-d8ad-4100-916b-d4d467615806",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3d262a8-b845-4023-83a6-5b37f8163ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:00<00:00, 1026.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Test Accuracy for user avg: 69.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 620/620 [00:00<00:00, 1013.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Test Accuracy for movie avg: 70.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline user average\n",
    "ratings_pred = test_removed.copy(deep=True)\n",
    "\n",
    "for user in tqdm(training_test[\"userId\"].unique()):\n",
    "    ratings_pred.loc[ratings_pred[\"userId\"] == user, [\"rating\"]] = np.digitize(training_test.loc[training_test.userId == user, \"userAvg\"].iat[0], bins=[3], right = False)\n",
    "\n",
    "avg_user_accuracy = accuracy_score(y_test, ratings_pred[\"rating\"])\n",
    "print(\"Baseline Test Accuracy for user avg: %.2f%%\" % (avg_user_accuracy * 100))\n",
    "\n",
    "# Baseline movie average\n",
    "ratings_pred = test_removed.copy(deep=True)\n",
    "training_movies = training_test.movieId.unique()\n",
    "\n",
    "for movie in tqdm(test_removed.movieId.unique()):\n",
    "    if movie in training_movies:\n",
    "        ratings_pred.loc[ratings_pred.movieId == movie, [\"rating\"]] = np.digitize(training_test.loc[training_test.movieId == movie, \"movieAvg\"].iat[0], bins=[3], right = False)\n",
    "    \n",
    "avg_movie_accuracy = accuracy_score(y_test, ratings_pred[\"rating\"])\n",
    "print(\"Baseline Test Accuracy for movie avg: %.2f%%\" % (avg_movie_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905dc50-d34c-4332-9843-153b266b40f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Regression Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "371179ff-46ad-421b-b1e5-14cae388fd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = copy.deepcopy(validation_removed['rating']) \n",
    "y_val = np.ravel(y_val)\n",
    "\n",
    "y_test = copy.deepcopy(test_removed['rating']) \n",
    "y_test = np.ravel(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a254eaf-77d3-4c9a-83c2-808742c9b7fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Validation Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8776be6-7393-4f0a-9b9a-46a1066d9dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:00<00:00, 974.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Validation RMSE for user avg: 0.970671907398001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 550/550 [00:00<00:00, 1029.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Validation RMSE for movie avg: 0.9279114110631916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline user average\n",
    "ratings_pred = validation_removed.copy(deep=True)\n",
    "for user in tqdm(validation_removed.userId.unique()):\n",
    "    ratings_pred.loc[ratings_pred[\"userId\"] == user, [\"rating\"]] = training_validation.loc[training_validation.userId == user, \"userAvg\"].iat[0]\n",
    "\n",
    "avg_user_rmse = mean_squared_error(y_val, ratings_pred[\"rating\"], squared = False)\n",
    "print(\"Baseline Validation RMSE for user avg:\", avg_user_rmse)\n",
    "\n",
    "# Baseline movie average\n",
    "ratings_pred = validation_removed.copy(deep=True)\n",
    "training_movies = training_validation.movieId.unique()\n",
    "\n",
    "for movie in tqdm(validation_removed.movieId.unique()):\n",
    "    if movie in training_movies:\n",
    "        ratings_pred.loc[ratings_pred.movieId == movie, [\"rating\"]] = training_validation.loc[training_validation.movieId == movie, \"movieAvg\"].iat[0]\n",
    "    \n",
    "avg_movie_rmse = mean_squared_error(y_val, ratings_pred[\"rating\"], squared = False)\n",
    "print(\"Baseline Validation RMSE for movie avg:\", avg_movie_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64dc73d-4dd4-4539-9768-aa708a052d27",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2e3e8f5-78eb-493d-b822-86bdac9bcf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:00<00:00, 1024.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Test RMSE for user avg: 0.9830622358141182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 620/620 [00:00<00:00, 1041.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Test RMSE for movie avg: 1.0358683519569263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline user average\n",
    "ratings_pred = test_removed.copy(deep=True)\n",
    "for user in tqdm(training_test[\"userId\"].unique()):\n",
    "    ratings_pred.loc[ratings_pred[\"userId\"] == user, [\"rating\"]] = training_test.loc[training_test.userId == user, \"userAvg\"].iat[0]\n",
    "\n",
    "avg_user_rmse = mean_squared_error(y_test, ratings_pred[\"rating\"], squared = False)\n",
    "print(\"Baseline Test RMSE for user avg:\", avg_user_rmse)\n",
    "\n",
    "# Baseline movie average\n",
    "ratings_pred = test_removed.copy(deep=True)\n",
    "training_movies = training_test.movieId.unique()\n",
    "\n",
    "for movie in tqdm(test_removed.movieId.unique()):\n",
    "    if movie in training_movies:\n",
    "        ratings_pred.loc[ratings_pred.movieId == movie, [\"rating\"]] = training_test.loc[training_test.movieId == movie, \"movieAvg\"].iat[0]\n",
    "    \n",
    "avg_movie_rmse = mean_squared_error(y_test, ratings_pred[\"rating\"], squared = False)\n",
    "print(\"Baseline Test RMSE for movie avg:\", avg_movie_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131420a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6176a46-f110-4983-abfa-3a8cea65a0b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e10650-32a0-4a99-8c28-7fcba0b735b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Classification Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d439fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mlxtend.evaluate import bootstrap_point632_score\n",
    "\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.tree import plot_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f8595a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = ['userId', 'movieId', 'userAvg', 'movieAvg', 'timestamp']\n",
    "use_cols.extend(genres)\n",
    "x_train = copy.deepcopy(training_validation[use_cols].to_numpy())\n",
    "y_train = copy.deepcopy(training_validation[['rating']])\n",
    "y_train = np.ravel(y_train)\n",
    "y_train = pd.cut(y_train, bins=[0,3,5], labels=[0,1]) # convert continuous to categorical\n",
    "\n",
    "x_val = copy.deepcopy(validation_removed[use_cols].to_numpy())\n",
    "y_val = copy.deepcopy(validation_removed['rating']) \n",
    "y_val = np.ravel(y_val)\n",
    "y_val = pd.cut(y_val, bins=[0,3,5], labels=[0,1]) # convert continuous to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c950021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=123),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [3, 4, 5, 6, 7, 8, 9, 10, 15, 20, None]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(random_state=123)\n",
    "param_grid = {'max_depth':[3,4,5,6,7,8,9,10,15,20,None], \n",
    "              'criterion':['gini','entropy']} \n",
    "\n",
    "gs = GridSearchCV(estimator=tree,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  #scoring='neg_root_mean_squared_error',\n",
    "                  cv=10,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c62cc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'criterion': 'gini', 'max_depth': 4}\n",
      "Best Validation Accuracy: 73.40%\n"
     ]
    }
   ],
   "source": [
    "print('Best Params: %s' % gs.best_params_)\n",
    "print('Best Validation Accuracy: %.2f%%' % (gs.best_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f413761-20c5-4358-a84d-0a3fb0fa3dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 76.02%\n"
     ]
    }
   ],
   "source": [
    "# Fit best model on whole dataset and predict validation data\n",
    "tree = DecisionTreeClassifier(**gs.best_params_, random_state=123)\n",
    "tree.fit(x_train, y_train)\n",
    "\n",
    "ratings_pred = tree.predict(x_val)\n",
    "accuracy = accuracy_score(y_val, ratings_pred)\n",
    "print('Validation Accuracy: %.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d56094d0-e8e7-4f1b-bbbb-067bddb55a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 75.91%\n"
     ]
    }
   ],
   "source": [
    "# Fit best model on whole dataset and predict test data\n",
    "x_train = copy.deepcopy(training_test[use_cols].to_numpy())\n",
    "\n",
    "y_train = copy.deepcopy(training_test[['rating']])\n",
    "y_train = np.ravel(y_train)\n",
    "y_train = pd.cut(y_train, bins=[0,3,5], labels=[0,1])\n",
    "\n",
    "x_test = copy.deepcopy(test_removed[use_cols].to_numpy())\n",
    "y_test = copy.deepcopy(test_removed['rating']) \n",
    "\n",
    "y_test = np.ravel(y_test)\n",
    "y_test = pd.cut(y_test, bins=[0,3,5], labels=[0,1]) # convert continuous to categorical\n",
    "\n",
    "tree = DecisionTreeClassifier(**gs.best_params_, random_state=123)\n",
    "tree.fit(x_train, y_train)\n",
    "\n",
    "ratings_pred = tree.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, ratings_pred)\n",
    "print('Test Accuracy: %.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9105b17a-bd76-4441-8994-e49b5b90c426",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Regression Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1bcd6727-37ec-4e1a-9a85-29e314fd8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = ['userId', 'movieId', 'userAvg', 'movieAvg', 'timestamp']\n",
    "use_cols.extend(genres)\n",
    "x_train = copy.deepcopy(training_validation[use_cols].to_numpy())\n",
    "y_train = copy.deepcopy(training_validation[['rating']])\n",
    "y_train = np.ravel(y_train)\n",
    "\n",
    "x_val = copy.deepcopy(validation_removed[use_cols].to_numpy())\n",
    "y_val = copy.deepcopy(validation_removed['rating']) \n",
    "y_val = np.ravel(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8ffe4ea-7732-4ac7-968f-70b64f0960ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=DecisionTreeRegressor(random_state=123),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10, 15, 20, None]},\n",
       "             scoring='neg_root_mean_squared_error')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(random_state=123)\n",
    "param_grid = {'max_depth':[3,4,5,6,7,8,9,10,15,20,None]} \n",
    "\n",
    "gs = GridSearchCV(estimator=tree,\n",
    "                  param_grid=param_grid,\n",
    "                  #scoring='neg_mean_absolute_percentage_error',\n",
    "                  scoring='neg_root_mean_squared_error',\n",
    "                  cv=10,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f78e7eb5-73b3-418e-9925-aad873b5de18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'max_depth': 6}\n",
      "Best Validation RMSE: 0.8342672177840438\n"
     ]
    }
   ],
   "source": [
    "print('Best Params:',  gs.best_params_)\n",
    "print('Best Validation RMSE:', -1 * gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10f11ede-d5b6-480c-8f46-c67cb84644f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.9203161079642824\n"
     ]
    }
   ],
   "source": [
    "# Fit best model on whole dataset and predict validation data\n",
    "tree = DecisionTreeRegressor(**gs.best_params_, random_state=123)\n",
    "tree.fit(x_train, y_train)\n",
    "\n",
    "ratings_pred = tree.predict(x_val)\n",
    "rmse = mean_squared_error(y_val, ratings_pred, squared = False)\n",
    "print('Validation RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4defd3f4-1240-4931-a580-1480fd0a60bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.903037475964926\n"
     ]
    }
   ],
   "source": [
    "# Fit best model on whole dataset and predict test data\n",
    "x_train = copy.deepcopy(training_test[use_cols].to_numpy())\n",
    "y_train = copy.deepcopy(training_test[['rating']])\n",
    "y_train = np.ravel(y_train)\n",
    "\n",
    "x_test = copy.deepcopy(test_removed[use_cols].to_numpy())\n",
    "y_test = copy.deepcopy(test_removed['rating']) \n",
    "y_test = np.ravel(y_test)\n",
    "\n",
    "tree = DecisionTreeRegressor(**gs.best_params_, random_state=123)\n",
    "tree.fit(x_train, y_train)\n",
    "\n",
    "ratings_pred = tree.predict(x_test)\n",
    "rmse = mean_squared_error(y_test, ratings_pred, squared = False)\n",
    "print('Test RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7a852e-bcbe-48b1-a1e3-c9c2af8569d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ebf457e-5329-487d-99ab-f79dd4c1f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdf6915-7db5-4df4-b0db-73e5287395e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Classification Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70add623-61c3-44e2-ac3a-fdd36060e967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 78.07%\n"
     ]
    }
   ],
   "source": [
    "# Fit best model on whole dataset and predict validation data\n",
    "use_cols = ['userId', 'movieId', 'userAvg', 'movieAvg', 'timestamp']\n",
    "use_cols.extend(genres)\n",
    "x_train = copy.deepcopy(training_validation[use_cols].to_numpy())\n",
    "\n",
    "y_train = copy.deepcopy(training_validation[['rating']])\n",
    "y_train = np.ravel(y_train)\n",
    "y_train = pd.cut(y_train, bins=[0,3,5], labels=[0,1]) # convert continuous to categorical\n",
    "\n",
    "x_val = copy.deepcopy(validation_removed[use_cols].to_numpy())\n",
    "\n",
    "y_val = copy.deepcopy(validation_removed['rating']) \n",
    "y_val = np.ravel(y_val)\n",
    "y_val = pd.cut(y_val, bins=[0,3,5], labels=[0,1]) # convert continuous to categorical\n",
    "\n",
    "clf = XGBClassifier(eval_metric = 'logloss', eta=0.1, max_depth=5, min_child_weight = 4, n_estimators = 200, subsample = 0.70, n_jobs=-1, random_state=1, use_label_encoder = False)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "ratings_pred = clf.predict(x_val)\n",
    "accuracy = accuracy_score(y_val, ratings_pred)\n",
    "print('Validation Accuracy: %.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42d81732-c01f-4342-9edb-f88c4082e6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 77.45%\n"
     ]
    }
   ],
   "source": [
    "# Fit best model on whole dataset and predict test data\n",
    "x_train = copy.deepcopy(training_test[use_cols].to_numpy())\n",
    "\n",
    "y_train = copy.deepcopy(training_test[['rating']])\n",
    "y_train = np.ravel(y_train)\n",
    "y_train = pd.cut(y_train, bins=[0,3,5], labels=[0,1])\n",
    "\n",
    "x_test = copy.deepcopy(test_removed[use_cols].to_numpy())\n",
    "y_test = copy.deepcopy(test_removed['rating']) \n",
    "\n",
    "y_test = np.ravel(y_test)\n",
    "y_test = pd.cut(y_test, bins=[0,3,5], labels=[0,1]) # convert continuous to categorical\n",
    "\n",
    "clf = XGBClassifier(eval_metric = 'logloss', eta=0.1, max_depth=5, min_child_weight = 4, n_estimators = 200, subsample = 0.70, n_jobs=-1, random_state=1, use_label_encoder = False)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "ratings_pred = clf.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, ratings_pred)\n",
    "print('Test Accuracy: %.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfd9b3d-8ce9-4cee-818a-880c85fa8fd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Regression Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "385cb6ac-b1ac-48e0-a780-18d5782a8db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.9038550284257149\n"
     ]
    }
   ],
   "source": [
    "# Fit best model on whole dataset and predict validation data\n",
    "use_cols = ['userId', 'movieId', 'userAvg', 'movieAvg', 'timestamp']\n",
    "use_cols.extend(genres)\n",
    "x_train = copy.deepcopy(training_validation[use_cols].to_numpy())\n",
    "\n",
    "y_train = copy.deepcopy(training_validation[['rating']])\n",
    "y_train = np.ravel(y_train)\n",
    "\n",
    "x_val = copy.deepcopy(validation_removed[use_cols].to_numpy())\n",
    "y_val = copy.deepcopy(validation_removed['rating']) \n",
    "y_val = np.ravel(y_val)\n",
    "\n",
    "clf = XGBRegressor(eval_metric = 'logloss', eta=0.1, max_depth=5, min_child_weight = 4, n_estimators = 200, subsample = 0.70, n_jobs=-1, random_state=1, use_label_encoder = False)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "ratings_pred = clf.predict(x_val)\n",
    "rmse = mean_squared_error(y_val, ratings_pred, squared = False)\n",
    "print('Validation RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "07aebc65-5594-4c56-b6ea-f8f0ba86662a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.8682323577760647\n"
     ]
    }
   ],
   "source": [
    "# Fit best model on whole dataset and predict validation data\n",
    "x_train = copy.deepcopy(training_test[use_cols].to_numpy())\n",
    "y_train = copy.deepcopy(training_test[['rating']])\n",
    "y_train = np.ravel(y_train)\n",
    "\n",
    "x_test = copy.deepcopy(test_removed[use_cols].to_numpy())\n",
    "y_test = copy.deepcopy(test_removed['rating']) \n",
    "y_test = np.ravel(y_test)\n",
    "\n",
    "clf = XGBRegressor(eval_metric = 'logloss', eta=0.1, max_depth=5, min_child_weight = 4, n_estimators = 200, subsample = 0.70, n_jobs=-1, random_state=1, use_label_encoder = False)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "ratings_pred = clf.predict(x_test)\n",
    "rmse = mean_squared_error(y_test, ratings_pred, squared = False)\n",
    "print('Test RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859aca0a",
   "metadata": {},
   "source": [
    "#### .632+ Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c6c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "scores = bootstrap_point632_score(gs, x_train, y_train, n_splits=50, \n",
    "                                  method='.632+', random_seed=123) \n",
    "acc = np.mean(scores)\n",
    "print('Accuracy: %.2f%%' % (100*acc))\n",
    "\n",
    "# 95% confidence interval\n",
    "lower = np.percentile(scores,2.5)*100\n",
    "upper = np.percentile(scores,97.5)*100\n",
    "print('95%% Confidence interval: [%.2f, %.2f]' % (lower, upper))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e462baef",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f14113",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = DecisionTreeClassifier(**gs.best_params_, random_state=123)\n",
    "best_model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a30a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plot_tree(best_model, \n",
    "          filled=True, \n",
    "          rounded=True,\n",
    "          class_names=['0', \n",
    "                       '1'],\n",
    "          feature_names=training_test[use_cols].columns) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556dc5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_estimator = gs.best_estimator_\n",
    "tree_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afebd08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_estimator.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac31d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_values = tree_estimator.feature_importances_\n",
    "feature_importances = pd.Series(feature_importance_values, index=training_test[use_cols].columns) # x train's column values\n",
    "feature_top20 = feature_importances.sort_values(ascending=False)[:20]\n",
    "\n",
    "plt.figure(figsize=[8, 6])\n",
    "plt.title('Feature Importances Top 20')\n",
    "sns.barplot(x=feature_top20, y=feature_top20.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0079a9",
   "metadata": {},
   "source": [
    "- movieAverage was the most important feature in the prediction model\n",
    "    - movie's quality/social standard in general affects individuals' rate \n",
    "- userAverage was the second > so each person's rating standard/rating tendancy affects actual rate\n",
    "    - someone could send out 0 for dislike and 3 or like but someone else could send 5 for all movies\n",
    "    \n",
    "- timestamp represents seconds that have elapsed since the Unix epoch 00:00:00 UTC on midnight UTC of Janurary 1, 1970\n",
    "    - https://en.wikipedia.org/wiki/Unix_time\n",
    "    - Thus when the movie was watched also affects the rate\n",
    "    \n",
    "- genres in the other hand doesn't significantly affect the rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b14d2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_decision_regions(np.asarray(copy.deepcopy(training_test[plot_cols])).astype('float32'), np.array(copy.deepcopy(y_train)), gs)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
