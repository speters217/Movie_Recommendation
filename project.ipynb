{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f0cafc0-1993-411c-9247-d75ac091280b",
   "metadata": {},
   "source": [
    "# Performance Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67ce4a31-1781-4f46-a7ce-e2b1f6cedc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost     : 1.5.1\n",
      "pandas      : 1.3.4\n",
      "numpy       : 1.21.3\n",
      "scikit-learn: 0.0\n",
      "\n",
      "Python 3.8.12\n"
     ]
    }
   ],
   "source": [
    "import watermark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import copy\n",
    "import os\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import ray\n",
    "from tqdm import tqdm\n",
    "\n",
    "%reload_ext watermark\n",
    "%watermark -p xgboost,pandas,numpy,scikit-learn\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381c55c2-1ec2-43fc-8c66-4a2acbc4b857",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf2e92fa-1bf4-4435-a1f3-4e9613ec83d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        userId  movieId  rating   timestamp\n",
      "0            1        1     4.0   964982703\n",
      "1            1        3     4.0   964981247\n",
      "2            1        6     4.0   964982224\n",
      "3            1       47     5.0   964983815\n",
      "4            1       50     5.0   964982931\n",
      "...        ...      ...     ...         ...\n",
      "100831     610   166534     4.0  1493848402\n",
      "100832     610   168248     5.0  1493850091\n",
      "100833     610   168250     5.0  1494273047\n",
      "100834     610   168252     5.0  1493846352\n",
      "100835     610   170875     3.0  1493846415\n",
      "\n",
      "[100836 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read ratings df\n",
    "path = \"data_small\"\n",
    "use_large = False\n",
    "if (use_large):\n",
    "    path = \"data_large\"\n",
    "    ratings = pd.read_csv('./' + path + '/ratings.csv')[:5000000]\n",
    "else:\n",
    "    ratings = pd.read_csv('./' + path + '/ratings.csv')\n",
    "\n",
    "movies = pd.read_csv('./' + path + '/movies.csv')\n",
    "\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37c3f66e-fe47-41bf-b960-2cdda6884d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9724/9724 [00:00<00:00, 249838.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time removing movies: 0:00:00.118261\n",
      "There were 8674 movies with fewer than 25 ratings. They have been removed from the dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Delete movies with fewer than 25 ratings\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "data = ratings.pivot(index = \"userId\", columns = \"movieId\", values = \"rating\")\n",
    "to_del = list()\n",
    "counts = data.count(axis = 0, numeric_only = True)\n",
    "for i in tqdm(data.columns.values):\n",
    "    if (counts[i] < 25):\n",
    "        to_del.append(i)\n",
    "ratings = ratings[~(ratings.movieId.isin(to_del))]\n",
    "del data\n",
    "\n",
    "print('Time removing movies: {}'.format(datetime.datetime.now()-start))\n",
    "print(\"There were\", len(to_del), \"movies with fewer than 25 ratings. They have been removed from the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ebd2d5d-c3c8-4930-8889-e41fc4088f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(610, 1050)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 610/610 [00:00<00:00, 203931.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time removing users: 0:00:00.027940\n",
      "There were 61 users with fewer than 20 ratings. They have been removed from the dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Delete users with fewer than 20 ratings\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "data = ratings.pivot(index = \"userId\", columns = \"movieId\", values = \"rating\")\n",
    "print(data.shape)\n",
    "to_del = list()\n",
    "counts = data.count(axis = 1, numeric_only = True)\n",
    "for i in tqdm(data.index.values):\n",
    "    if (counts[i] < 20):\n",
    "        to_del.append(i)\n",
    "ratings = ratings[~(ratings.userId.isin(to_del))]\n",
    "del data\n",
    "\n",
    "print('Time removing users: {}'.format(datetime.datetime.now()-start))\n",
    "print(\"There were\", len(to_del), \"users with fewer than 20 ratings. They have been removed from the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7222a454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Removes users who no longer have 20 ratings\\n@ray.remote\\ndef insufficient_ratings(dataset, user):\\n    user_ratings = sum(dataset.userId == user)\\n    if (user_ratings < 20):\\n        return user\\n    return -1\\n\\ndef to_iterator(obj_ids):\\n    while obj_ids:\\n        done, obj_ids = ray.wait(obj_ids)\\n        yield ray.get(done[0])\\n\\ndat = ray.put(ratings)\\nobj_ids = [insufficient_ratings.remote(dat, user) for user in ratings.userId.unique()]\\nresults = []\\nfor x in tqdm(to_iterator(obj_ids), total = len(obj_ids)):\\n    if x > -1:\\n        results.append(x)\\n\\nratings = ratings.loc[~(ratings.userId.isin(results))]\\n\\ndel dat\\n        \\nprint(\"There were\", len(results), \"users with fewer than 20 ratings. They have been removed from the dataset.\")\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removes users with fewer than 20 ratings\n",
    "# To be used when the userXmovie matrix is too large\n",
    "\"\"\"\n",
    "# Removes users who no longer have 20 ratings\n",
    "@ray.remote\n",
    "def insufficient_ratings(dataset, user):\n",
    "    user_ratings = sum(dataset.userId == user)\n",
    "    if (user_ratings < 20):\n",
    "        return user\n",
    "    return -1\n",
    "\n",
    "def to_iterator(obj_ids):\n",
    "    while obj_ids:\n",
    "        done, obj_ids = ray.wait(obj_ids)\n",
    "        yield ray.get(done[0])\n",
    "\n",
    "dat = ray.put(ratings)\n",
    "obj_ids = [insufficient_ratings.remote(dat, user) for user in ratings.userId.unique()]\n",
    "results = []\n",
    "for x in tqdm(to_iterator(obj_ids), total = len(obj_ids)):\n",
    "    if x > -1:\n",
    "        results.append(x)\n",
    "\n",
    "ratings = ratings.loc[~(ratings.userId.isin(results))]\n",
    "\n",
    "del dat\n",
    "        \n",
    "print(\"There were\", len(results), \"users with fewer than 20 ratings. They have been removed from the dataset.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8bfdee0-c446-44c4-a28f-7f08776585db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep 20 ratings per user\n",
    "#ratings = ratings.groupby(\"userId\").sample(n=20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c39be7f4-5f67-4d6f-b5f2-0470eb9ee31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9742/9742 [00:00<00:00, 1221040.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(no genres listed)', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'IMAX', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all genres\n",
    "genres_set = set()\n",
    "for string in tqdm(movies.genres):\n",
    "    genres_set.update(string.split('|'))\n",
    "#genres.remove('(no genres listed)')\n",
    "genres = sorted(genres_set)\n",
    "print(genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62c88906-630e-4cc3-869d-984598f09887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1050/1050 [00:02<00:00, 368.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# Add one-hot encoding for movie genres\n",
    "for genre in genres:\n",
    "    ratings[genre] = False\n",
    "\n",
    "for movie in tqdm(ratings.movieId.unique()):\n",
    "    for genre in movies.genres[movies.loc[movies.movieId == movie].index[0]].split('|'):\n",
    "        ratings.loc[ratings.movieId == movie, [genre]] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27ad3e85-a683-40e8-b334-8e1da4936a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Valid/Test sizes: 38700 10195 12724\n",
      "Train/Valid/Test users: 351 88 110\n"
     ]
    }
   ],
   "source": [
    "# Note that GroupShuffleSplit allows us to have distinct users in the training, validation, and test sets\n",
    "\n",
    "# Split into 80-20 training - testing split\n",
    "train_inds, test_inds = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 42).split(ratings, groups=ratings.userId))\n",
    "\n",
    "train_val = ratings.iloc[train_inds].copy()\n",
    "test = ratings.iloc[test_inds].copy()\n",
    "\n",
    "# Split into 80-20 training - validation split\n",
    "train_inds, val_inds = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 42).split(train_val, groups=train_val.userId))\n",
    "\n",
    "training = train_val.iloc[train_inds].copy()\n",
    "validation = train_val.iloc[val_inds].copy()\n",
    "\n",
    "print('Train/Valid/Test sizes:', training.shape[0], validation.shape[0], test.shape[0])\n",
    "print('Train/Valid/Test users:', len(training.userId.unique()), len(validation.userId.unique()), len(test.userId.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec799c23-52a6-404f-927a-96b54d7964bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As it is, this will take the 10 most recent ratings for a user to use for prediction\n",
    "# Commented out sections allow random ratings to be taken\n",
    "\n",
    "@ray.remote\n",
    "def prepare_helper(user, dataset):\n",
    "    #random.seed(1)\n",
    "    user_ratings = dataset.loc[dataset.userId == user]\n",
    "                               \n",
    "    # Randomly select 10 movies to remove\n",
    "    #selected_movies = random.sample(range(0, user_ratings.shape[0]), 10)\n",
    "    #selected_movies = random.choices(list(user_ratings.movieId), k=10)\n",
    "                               \n",
    "    # This will select the 10 most recent ratings for this user\n",
    "    selected_movies = user_ratings.sort_values(by = \"timestamp\", ascending = False).movieId.values[0:10]\n",
    "    \n",
    "    # Add the removed movies to the removed dataframe\n",
    "    removed_movies = user_ratings[user_ratings.movieId.isin(selected_movies)]\n",
    "    \n",
    "    return removed_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dd65052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in either the validation or training set\n",
    "# Removes the most recent 10 ratings for each user, and returns them as removed_movies\n",
    "# Returns the original dataset with the ratings to predict removed as new_movies\n",
    "def prepare_data(dataset):\n",
    "    # For validation and test data, remove 10 rated movies to be predicted\n",
    "    removed_movies = pd.DataFrame(columns = dataset.columns)\n",
    "    # Iterate over each user\n",
    "    \n",
    "    def to_iterator(obj_ids):\n",
    "        while obj_ids:\n",
    "            done, obj_ids = ray.wait(obj_ids)\n",
    "            yield ray.get(done[0])\n",
    "    dat = ray.put(dataset)\n",
    "    obj_ids = [prepare_helper.remote(user, dat) for user in dataset[\"userId\"].unique()]\n",
    "    results = []\n",
    "    for x in tqdm(to_iterator(obj_ids), total = len(obj_ids)):\n",
    "        results.append(x)\n",
    "    \n",
    "    # Combine removed movies:\n",
    "    for result in tqdm(results):\n",
    "        removed_movies = pd.concat([removed_movies, result])\n",
    "        \n",
    "    # Remove all of the user-movie combinations in removed_moviews from dataset\n",
    "    new_movies = pd.merge(dataset, removed_movies, indicator=True, how='outer').query('_merge==\"left_only\"').drop('_merge', axis=1)\n",
    "    del dat\n",
    "    \n",
    "    return removed_movies, new_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e73b9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:00<00:00, 95.50it/s]\n",
      "100%|██████████| 88/88 [00:00<00:00, 811.44it/s]\n",
      "2021-12-05 09:42:34,165\tINFO worker.py:832 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time building validation set: 0:00:13.587005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 2005.39it/s]\n",
      "100%|██████████| 110/110 [00:00<00:00, 720.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time building test set: 0:00:00.429375\n"
     ]
    }
   ],
   "source": [
    "# Get the proper splits for the validation and testing sets\n",
    "if (os.path.isfile('./' + path + '/validation_removed_tmp.csv') and os.path.isfile('./' + path + '/validation_new.csv')):\n",
    "    print(\"Validation set already exists\" )\n",
    "    validation_removed = pd.read_csv('./' + path + '/validation_removed_tmp.csv')\n",
    "    validation_new = pd.read_csv('./' + path + '/validation_new.csv')\n",
    "else:\n",
    "    start = datetime.datetime.now()\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "    validation_removed, validation_new = prepare_data(validation)\n",
    "    print('Time building validation set: {}'.format(datetime.datetime.now()-start))\n",
    "    validation_removed.to_csv('./' + path + '/validation_removed_tmp.csv')\n",
    "    validation_new.to_csv('./' + path + '/validation_new.csv')\n",
    "\n",
    "if (os.path.isfile('./' + path + '/test_removed_tmp.csv') and os.path.isfile('./' + path + '/test_new.csv')):\n",
    "    print(\"Test set already exists\" )\n",
    "    test_removed = pd.read_csv('./' + path + '/test_removed_tmp.csv')\n",
    "    test_new = pd.read_csv('./' + path + '/test_new.csv')                                                                \n",
    "else:\n",
    "    start = datetime.datetime.now()\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "    test_removed, test_new = prepare_data(test)\n",
    "    print('Time building test set: {}'.format(datetime.datetime.now()-start))\n",
    "    test_removed.to_csv('./' + path + '/test_removed_tmp.csv')\n",
    "    test_new.to_csv('./' + path + '/test_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca37d3e9-b046-4f58-956f-e997561c41a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns dataset with the average user rating for each user\n",
    "# Optionally takes in removed when using validation or test data\n",
    "\"\"\"\n",
    "# Return tuple of (user, average rating for user)\n",
    "@ray.remote\n",
    "def add_user(dataset, user):\n",
    "    return (user, np.mean(dataset[dataset[\"userId\"] == user][\"rating\"]))\n",
    "\n",
    "def user_avg(dataset, using_removed = False, removed = pd.DataFrame()):\n",
    "    def to_iterator(obj_ids):\n",
    "        while obj_ids:\n",
    "            done, obj_ids = ray.wait(obj_ids)\n",
    "            yield ray.get(done[0])\n",
    "\n",
    "    # Get the user averages\n",
    "    dataset[\"userAvg\"] = 0.0\n",
    "    dat = ray.put(dataset)\n",
    "    obj_ids = ([add_user.remote(dat, user) for user in dataset.userId.unique()])\n",
    "    results = []\n",
    "    for x in tqdm(to_iterator(obj_ids), total = len(obj_ids)):\n",
    "        results.append(x)\n",
    "    del dat\n",
    "\n",
    "    if (using_removed):\n",
    "        removed[\"userAvg\"] = 0.0\n",
    "    print(\"Combining data\")\n",
    "    for result in tqdm(results):\n",
    "        dataset.loc[dataset[\"userId\"] == result[0], [\"userAvg\"]] = result[1]\n",
    "        if (using_removed):\n",
    "            removed.loc[removed.userId == result[0], [\"userAvg\"]] = result[1]\n",
    "    \n",
    "    return dataset, removed\n",
    "\"\"\"\n",
    "def user_avg(dataset, using_removed = False, removed = pd.DataFrame()):\n",
    "    data = dataset.pivot(index = \"userId\", columns = \"movieId\", values = \"rating\")\n",
    "    # Get the movie averages\n",
    "    dataset[\"userAvg\"] = 0.0\n",
    "    if (using_removed):\n",
    "        removed[\"userAvg\"] = 0.0\n",
    "    #print(\"Combining data\")\n",
    "    averages = data.mean(axis = 1, numeric_only = True)\n",
    "    for user in tqdm(data.index.values):\n",
    "        dataset.loc[dataset.userId == user, [\"userAvg\"]] = averages[user]\n",
    "        if (using_removed):\n",
    "            removed.loc[removed.userId == user, [\"userAvg\"]] = averages[user]\n",
    "        \n",
    "    return dataset, removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12240444-0490-44ad-87df-7b93afc2ac4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef movie_avg(dataset, removed):\\n    data = dataset.pivot(index = \"userId\", columns = \"movieId\", values = \"rating\")\\n    # Get the movie averages\\n    dataset[\"movieAvg\"] = 0.0    \\n    removed[\"movieAvg\"] = 0.0\\n    #print(\"Combining data\")\\n    averages = data.mean(axis = 0, numeric_only = True)\\n    for movie in tqdm(data.columns.values):\\n        dataset.loc[dataset.movieId == movie, [\"movieAvg\"]] = averages[movie]\\n        removed.loc[removed.movieId == movie, [\"movieAvg\"]] = averages[movie]\\n        \\n    return dataset, removed\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns dataset with the average movie rating for each movie\n",
    "\n",
    "# This commented out section is the multiproccessing version, which appears to run slower\n",
    "\n",
    "# Return tuple of (movie, average rating for movie)\n",
    "\"\"\"\n",
    "@ray.remote\n",
    "def add_movie(dataset, movie):\n",
    "    return (movie, np.mean(dataset[dataset[\"movieId\"] == movie][\"rating\"]))\n",
    "\n",
    "def movie_avg(dataset, removed):\n",
    "    def to_iterator(obj_ids):\n",
    "        while obj_ids:\n",
    "            done, obj_ids = ray.wait(obj_ids)\n",
    "            yield ray.get(done[0])\n",
    "\n",
    "    # Get the movie averages\n",
    "    dataset[\"movieAvg\"] = 0.0\n",
    "    dat = ray.put(dataset)\n",
    "    obj_ids = ([add_movie.remote(dat, movie) for movie in dataset.movieId.unique()])\n",
    "    results = []\n",
    "    for x in tqdm(to_iterator(obj_ids), total = len(obj_ids)):\n",
    "        results.append(x)\n",
    "    del dat\n",
    "    \n",
    "    removed[\"movieAvg\"] = 0.0\n",
    "    print(\"Combining data\")\n",
    "    for result in tqdm(results):\n",
    "        dataset.loc[dataset.movieId == result[0], [\"movieAvg\"]] = result[1]\n",
    "        removed.loc[removed.movieId == result[0], [\"movieAvg\"]] = result[1]\n",
    "        \n",
    "    return dataset, removed\n",
    "\"\"\"\n",
    "def movie_avg(dataset, removed):\n",
    "    # Get the movie averages\n",
    "    dataset[\"movieAvg\"] = 0.0    \n",
    "    removed[\"movieAvg\"] = 0.0\n",
    "    for movie in tqdm(dataset.movieId.unique()):\n",
    "        rating = np.mean(dataset[dataset[\"movieId\"] == movie][\"rating\"])\n",
    "        dataset.loc[dataset.movieId == movie, [\"movieAvg\"]] = rating\n",
    "        removed.loc[removed.movieId == movie, [\"movieAvg\"]] = rating\n",
    "        \n",
    "    return dataset, removed\n",
    "\"\"\"\n",
    "def movie_avg(dataset, removed):\n",
    "    data = dataset.pivot(index = \"userId\", columns = \"movieId\", values = \"rating\")\n",
    "    # Get the movie averages\n",
    "    dataset[\"movieAvg\"] = 0.0    \n",
    "    removed[\"movieAvg\"] = 0.0\n",
    "    #print(\"Combining data\")\n",
    "    averages = data.mean(axis = 0, numeric_only = True)\n",
    "    for movie in tqdm(data.columns.values):\n",
    "        dataset.loc[dataset.movieId == movie, [\"movieAvg\"]] = averages[movie]\n",
    "        removed.loc[removed.movieId == movie, [\"movieAvg\"]] = averages[movie]\n",
    "        \n",
    "    return dataset, removed\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f67d751-3753-4522-b63c-b22a7e550785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return datasets with average movie rating and average user rating for each row\n",
    "def add_avgs(training, validation_new, test_new, validation_removed, test_removed):\n",
    "    print(\"\\nCalculating training set user averages\")\n",
    "    training, dummy = user_avg(training, False)\n",
    "    print(\"\\nCalculating validation set user averages\")\n",
    "    validation_new, validation_removed = user_avg(validation_new, True, validation_removed)\n",
    "    print(\"\\nCalculating test set user averages\")\n",
    "    test_new, test_removed = user_avg(test_new, True, test_removed)\n",
    "    \n",
    "    print(\"\\nCalculating validation set movie averages\")\n",
    "    training_validation, validation_removed = movie_avg(pd.concat([training, validation_new]), validation_removed)\n",
    "    \n",
    "    print(\"\\nCalculating test set movie averages\")\n",
    "    training_test, test_removed = movie_avg(pd.concat([training, test_new]), test_removed)\n",
    "    \n",
    "    return training_validation, training_test, validation_removed, test_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d39ff1ce-2647-43a2-a457-24f225c47744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating training set user averages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 351/351 [00:00<00:00, 703.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating validation set user averages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:00<00:00, 193.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating test set user averages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 208.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating validation set movie averages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1050/1050 [00:10<00:00, 100.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating test set movie averages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1050/1050 [00:09<00:00, 107.41it/s]\n"
     ]
    }
   ],
   "source": [
    "if (os.path.isfile('./' + path + '/training_validation.csv') and os.path.isfile('./' + path + '/training_test.csv') and os.path.isfile('./' + path + '/validation_removed.csv') and os.path.isfile('./' + path + '/test_removed.csv')):\n",
    "    print(\"Sets already exist\" )\n",
    "    training_validation = pd.read_csv('./' + path + '/training_validation.csv')\n",
    "    training_test = pd.read_csv('./' + path + '/training_test.csv')\n",
    "    validation_removed = pd.read_csv('./' + path + '/validation_removed.csv')\n",
    "    test_removed = pd.read_csv('./' + path + '/test_removed.csv')\n",
    "else:\n",
    "    \n",
    "    # training_validation has all training data as well as the unremoved values in the validation data\n",
    "    # this will be used to train, and we will use the removed validation data to predict\n",
    "    \n",
    "    # training_test has all training data as well as the unremoved values in the test data\n",
    "    # this will be used to train, and we will use the removed test data to predict\n",
    "    \n",
    "    training_validation, training_test, validation_removed, test_removed = add_avgs(training, validation_new, test_new, validation_removed, test_removed)\n",
    "    training_validation.to_csv('./' + path + '/training_validation.csv')\n",
    "    training_test.to_csv('./' + path + '/training_test.csv')\n",
    "    validation_removed.to_csv('./' + path + '/validation_removed.csv')\n",
    "    test_removed.to_csv('./' + path + '/test_removed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e7c027-a164-4969-8e62-4f9d2e629575",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19408600-e639-4c11-b82a-4e60369b8c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac59bb82-103d-4bef-a865-fd949778126e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classification Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9a52056-52cc-405b-8ce9-11d16e87fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = copy.deepcopy(validation_removed['rating']) \n",
    "y_val = np.ravel(y_val)\n",
    "y_val = pd.cut(y_val, bins=[0,3,5], labels=[0,1]) # convert continuous to categorical\n",
    "\n",
    "y_test = copy.deepcopy(test_removed['rating']) \n",
    "y_test = np.ravel(y_test)\n",
    "y_test = pd.cut(y_test, bins=[0,3,5], labels=[0,1]) # convert continuous to categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266605ec-ea31-4f38-917d-00dae2d05890",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Validation Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0378a33-cf1f-44df-8e59-145710b9e6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439/439 [00:02<00:00, 198.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Validation Accuracy for user avg: 71.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 550/550 [00:02<00:00, 210.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Test Accuracy for movie avg: 72.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline user average\n",
    "ratings_pred = validation_removed.copy(deep=True)\n",
    "\n",
    "for user in tqdm(training_validation[\"userId\"].unique()):\n",
    "    ratings_pred.loc[ratings_pred[\"userId\"] == user, [\"rating\"]] = np.digitize(training_validation.loc[training_validation.userId == user, \"userAvg\"].iat[0], bins=[3], right = False)\n",
    "\n",
    "avg_user_accuracy = accuracy_score(y_val, ratings_pred[\"rating\"])\n",
    "print(\"Baseline Validation Accuracy for user avg: %.2f%%\" % (avg_user_accuracy * 100))\n",
    "\n",
    "# Baseline movie average\n",
    "ratings_pred = validation_removed.copy(deep=True)\n",
    "training_movies = training_validation.movieId.unique()\n",
    "\n",
    "for movie in tqdm(validation_removed.movieId.unique()):\n",
    "    if movie in training_movies:\n",
    "        ratings_pred.loc[ratings_pred.movieId == movie, [\"rating\"]] = np.digitize(training_validation.loc[training_validation.movieId == movie, \"movieAvg\"].iat[0], bins=[3], right = False)\n",
    "    # If this movie happens to not be in the training data, simply assign a 0 (not like)\n",
    "    else:\n",
    "        ratings_pred.loc[ratings_pred.movieId == movie, [\"rating\"]] = 0\n",
    "    \n",
    "avg_movie_accuracy = accuracy_score(y_val, ratings_pred[\"rating\"])\n",
    "print(\"Baseline Test Accuracy for movie avg: %.2f%%\" % (avg_movie_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d8b782-d8ad-4100-916b-d4d467615806",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3d262a8-b845-4023-83a6-5b37f8163ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:02<00:00, 196.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Test Accuracy for user avg: 69.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 620/620 [00:02<00:00, 212.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Test Accuracy for movie avg: 70.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline user average\n",
    "ratings_pred = test_removed.copy(deep=True)\n",
    "\n",
    "for user in tqdm(training_test[\"userId\"].unique()):\n",
    "    ratings_pred.loc[ratings_pred[\"userId\"] == user, [\"rating\"]] = np.digitize(training_test.loc[training_test.userId == user, \"userAvg\"].iat[0], bins=[3], right = False)\n",
    "\n",
    "avg_user_accuracy = accuracy_score(y_test, ratings_pred[\"rating\"])\n",
    "print(\"Baseline Test Accuracy for user avg: %.2f%%\" % (avg_user_accuracy * 100))\n",
    "\n",
    "# Baseline movie average\n",
    "ratings_pred = test_removed.copy(deep=True)\n",
    "training_movies = training_test.movieId.unique()\n",
    "\n",
    "for movie in tqdm(test_removed.movieId.unique()):\n",
    "    if movie in training_movies:\n",
    "        ratings_pred.loc[ratings_pred.movieId == movie, [\"rating\"]] = np.digitize(training_test.loc[training_test.movieId == movie, \"movieAvg\"].iat[0], bins=[3], right = False)\n",
    "    # If this movie happens to not be in the training data, simply assign a 0 (not like)\n",
    "    else:\n",
    "        ratings_pred.loc[ratings_pred.movieId == movie, [\"rating\"]] = 0\n",
    "    \n",
    "avg_movie_accuracy = accuracy_score(y_test, ratings_pred[\"rating\"])\n",
    "print(\"Baseline Test Accuracy for movie avg: %.2f%%\" % (avg_movie_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905dc50-d34c-4332-9843-153b266b40f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Regression Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "371179ff-46ad-421b-b1e5-14cae388fd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = copy.deepcopy(validation_removed['rating']) \n",
    "y_val = np.ravel(y_val)\n",
    "\n",
    "y_test = copy.deepcopy(test_removed['rating']) \n",
    "y_test = np.ravel(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a254eaf-77d3-4c9a-83c2-808742c9b7fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Validation Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8776be6-7393-4f0a-9b9a-46a1066d9dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:00<00:00, 216.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Validation RMSE for user avg: 0.970671907398001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 550/550 [00:02<00:00, 224.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Validation RMSE for movie avg: 0.9279114110631916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline user average\n",
    "ratings_pred = validation_removed.copy(deep=True)\n",
    "for user in tqdm(validation_removed.userId.unique()):\n",
    "    ratings_pred.loc[ratings_pred[\"userId\"] == user, [\"rating\"]] = training_validation.loc[training_validation.userId == user, \"userAvg\"].iat[0]\n",
    "\n",
    "avg_user_rmse = mean_squared_error(y_val, ratings_pred[\"rating\"], squared = False)\n",
    "print(\"Baseline Validation RMSE for user avg:\", avg_user_rmse)\n",
    "\n",
    "# Baseline movie average\n",
    "ratings_pred = validation_removed.copy(deep=True)\n",
    "training_movies = training_validation.movieId.unique()\n",
    "\n",
    "for movie in tqdm(validation_removed.movieId.unique()):\n",
    "    if movie in training_movies:\n",
    "        ratings_pred.loc[ratings_pred.movieId == movie, [\"rating\"]] = training_validation.loc[training_validation.movieId == movie, \"movieAvg\"].iat[0]\n",
    "    # If this movie happens to not be in the training data, simply assign a 0 (not like)\n",
    "    else:\n",
    "        ratings_pred.loc[ratings_pred.movieId == movie, [\"rating\"]] = 0\n",
    "    \n",
    "avg_movie_rmse = mean_squared_error(y_val, ratings_pred[\"rating\"], squared = False)\n",
    "print(\"Baseline Validation RMSE for movie avg:\", avg_movie_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64dc73d-4dd4-4539-9768-aa708a052d27",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2e3e8f5-78eb-493d-b822-86bdac9bcf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:02<00:00, 218.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Test RMSE for user avg: 0.9830622358141182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 620/620 [00:02<00:00, 232.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Test RMSE for movie avg: 1.0358683519569263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline user average\n",
    "ratings_pred = test_removed.copy(deep=True)\n",
    "for user in tqdm(training_test[\"userId\"].unique()):\n",
    "    ratings_pred.loc[ratings_pred[\"userId\"] == user, [\"rating\"]] = training_test.loc[training_test.userId == user, \"userAvg\"].iat[0]\n",
    "\n",
    "avg_user_rmse = mean_squared_error(y_test, ratings_pred[\"rating\"], squared = False)\n",
    "print(\"Baseline Test RMSE for user avg:\", avg_user_rmse)\n",
    "\n",
    "# Baseline movie average\n",
    "ratings_pred = test_removed.copy(deep=True)\n",
    "training_movies = training_test.movieId.unique()\n",
    "\n",
    "for movie in tqdm(test_removed.movieId.unique()):\n",
    "    if movie in training_movies:\n",
    "        ratings_pred.loc[ratings_pred.movieId == movie, [\"rating\"]] = training_test.loc[training_test.movieId == movie, \"movieAvg\"].iat[0]\n",
    "    # If this movie happens to not be in the training data, simply assign a 0 (not like)\n",
    "    else:\n",
    "        ratings_pred.loc[ratings_pred.movieId == movie, [\"rating\"]] = 0\n",
    "    \n",
    "avg_movie_rmse = mean_squared_error(y_test, ratings_pred[\"rating\"], squared = False)\n",
    "print(\"Baseline Test RMSE for movie avg:\", avg_movie_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131420a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6176a46-f110-4983-abfa-3a8cea65a0b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e10650-32a0-4a99-8c28-7fcba0b735b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Classification Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d439fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mlxtend.evaluate import bootstrap_point632_score\n",
    "\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.tree import plot_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f8595a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = ['userId', 'movieId', 'userAvg', 'movieAvg', 'timestamp']\n",
    "use_cols.extend(genres)\n",
    "x_train = copy.deepcopy(training_validation[use_cols].to_numpy())\n",
    "y_train = copy.deepcopy(training_validation[['rating']])\n",
    "y_train = np.ravel(y_train)\n",
    "y_train = pd.cut(y_train, bins=[0,3,5], labels=[0,1]) # convert continuous to categorical\n",
    "\n",
    "x_val = copy.deepcopy(validation_removed[use_cols].to_numpy())\n",
    "y_val = copy.deepcopy(validation_removed['rating']) \n",
    "y_val = np.ravel(y_val)\n",
    "y_val = pd.cut(y_val, bins=[0,3,5], labels=[0,1]) # convert continuous to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c950021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 22 candidates, totalling 220 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=123),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [3, 4, 5, 6, 7, 8, 9, 10, 15, 20, None]},\n",
       "             scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(random_state=123)\n",
    "param_grid = {'max_depth':[3,4,5,6,7,8,9,10,15,20,None], \n",
    "              'criterion':['gini','entropy']} \n",
    "\n",
    "gs = GridSearchCV(estimator=tree,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv=10,\n",
    "                  n_jobs=-1,\n",
    "                  verbose=3)\n",
    "\n",
    "gs.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c62cc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'criterion': 'gini', 'max_depth': 4}\n",
      "Best Validation Accuracy: 73.40%\n"
     ]
    }
   ],
   "source": [
    "print('Best Params: %s' % gs.best_params_)\n",
    "print('Best Validation Accuracy: %.2f%%' % (gs.best_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f413761-20c5-4358-a84d-0a3fb0fa3dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 76.02%\n"
     ]
    }
   ],
   "source": [
    "# Fit best model on whole dataset and predict validation data\n",
    "tree = DecisionTreeClassifier(**gs.best_params_, random_state=123)\n",
    "tree.fit(x_train, y_train)\n",
    "\n",
    "ratings_pred = tree.predict(x_val)\n",
    "accuracy = accuracy_score(y_val, ratings_pred)\n",
    "print('Validation Accuracy: %.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d56094d0-e8e7-4f1b-bbbb-067bddb55a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 75.91%\n"
     ]
    }
   ],
   "source": [
    "# Fit best model on whole dataset and predict test data\n",
    "x_train = copy.deepcopy(training_test[use_cols].to_numpy())\n",
    "\n",
    "y_train = copy.deepcopy(training_test[['rating']])\n",
    "y_train = np.ravel(y_train)\n",
    "y_train = pd.cut(y_train, bins=[0,3,5], labels=[0,1])\n",
    "\n",
    "x_test = copy.deepcopy(test_removed[use_cols].to_numpy())\n",
    "y_test = copy.deepcopy(test_removed['rating']) \n",
    "\n",
    "y_test = np.ravel(y_test)\n",
    "y_test = pd.cut(y_test, bins=[0,3,5], labels=[0,1]) # convert continuous to categorical\n",
    "\n",
    "tree = DecisionTreeClassifier(**gs.best_params_, random_state=123)\n",
    "tree.fit(x_train, y_train)\n",
    "\n",
    "ratings_pred = tree.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, ratings_pred)\n",
    "print('Test Accuracy: %.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9105b17a-bd76-4441-8994-e49b5b90c426",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Regression Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1bcd6727-37ec-4e1a-9a85-29e314fd8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = ['userId', 'movieId', 'userAvg', 'movieAvg', 'timestamp']\n",
    "use_cols.extend(genres)\n",
    "x_train = copy.deepcopy(training_validation[use_cols].to_numpy())\n",
    "y_train = copy.deepcopy(training_validation[['rating']])\n",
    "y_train = np.ravel(y_train)\n",
    "\n",
    "x_val = copy.deepcopy(validation_removed[use_cols].to_numpy())\n",
    "y_val = copy.deepcopy(validation_removed['rating']) \n",
    "y_val = np.ravel(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8ffe4ea-7732-4ac7-968f-70b64f0960ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 11 candidates, totalling 110 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=DecisionTreeRegressor(random_state=123),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10, 15, 20, None]},\n",
       "             scoring='neg_root_mean_squared_error', verbose=3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(random_state=123)\n",
    "param_grid = {'max_depth':[3,4,5,6,7,8,9,10,15,20,None]} \n",
    "\n",
    "gs = GridSearchCV(estimator=tree,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='neg_root_mean_squared_error',\n",
    "                  cv=10,\n",
    "                  n_jobs=-1,\n",
    "                  verbose=3)\n",
    "\n",
    "gs.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f78e7eb5-73b3-418e-9925-aad873b5de18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'max_depth': 6}\n",
      "Best Validation RMSE: 0.8342672177840438\n"
     ]
    }
   ],
   "source": [
    "print('Best Params:',  gs.best_params_)\n",
    "print('Best Validation RMSE:', -1 * gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10f11ede-d5b6-480c-8f46-c67cb84644f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.9203161079642824\n"
     ]
    }
   ],
   "source": [
    "# Fit best model on whole dataset and predict validation data\n",
    "tree = DecisionTreeRegressor(**gs.best_params_, random_state=123)\n",
    "tree.fit(x_train, y_train)\n",
    "\n",
    "ratings_pred = tree.predict(x_val)\n",
    "rmse = mean_squared_error(y_val, ratings_pred, squared = False)\n",
    "print('Validation RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4defd3f4-1240-4931-a580-1480fd0a60bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.9030374759649261\n"
     ]
    }
   ],
   "source": [
    "# Fit best model on whole dataset and predict test data\n",
    "x_train = copy.deepcopy(training_test[use_cols].to_numpy())\n",
    "y_train = copy.deepcopy(training_test[['rating']])\n",
    "y_train = np.ravel(y_train)\n",
    "\n",
    "x_test = copy.deepcopy(test_removed[use_cols].to_numpy())\n",
    "y_test = copy.deepcopy(test_removed['rating']) \n",
    "y_test = np.ravel(y_test)\n",
    "\n",
    "tree = DecisionTreeRegressor(**gs.best_params_, random_state=123)\n",
    "tree.fit(x_train, y_train)\n",
    "\n",
    "ratings_pred = tree.predict(x_test)\n",
    "rmse = mean_squared_error(y_test, ratings_pred, squared = False)\n",
    "print('Test RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7a852e-bcbe-48b1-a1e3-c9c2af8569d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ebf457e-5329-487d-99ab-f79dd4c1f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdf6915-7db5-4df4-b0db-73e5287395e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Classification Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70add623-61c3-44e2-ac3a-fdd36060e967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 78.07%\n"
     ]
    }
   ],
   "source": [
    "# Fit best model on whole dataset and predict validation data\n",
    "use_cols = ['userId', 'movieId', 'userAvg', 'movieAvg', 'timestamp']\n",
    "use_cols.extend(genres)\n",
    "x_train = copy.deepcopy(training_validation[use_cols].to_numpy())\n",
    "\n",
    "y_train = copy.deepcopy(training_validation[['rating']])\n",
    "y_train = np.ravel(y_train)\n",
    "y_train = pd.cut(y_train, bins=[0,3,5], labels=[0,1]) # convert continuous to categorical\n",
    "\n",
    "x_val = copy.deepcopy(validation_removed[use_cols].to_numpy())\n",
    "\n",
    "y_val = copy.deepcopy(validation_removed['rating']) \n",
    "y_val = np.ravel(y_val)\n",
    "y_val = pd.cut(y_val, bins=[0,3,5], labels=[0,1]) # convert continuous to categorical\n",
    "\n",
    "clf = XGBClassifier(eval_metric = 'logloss', eta=0.1, max_depth=5, min_child_weight = 4, n_estimators = 200, subsample = 0.70, n_jobs=-1, random_state=1, use_label_encoder = False)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "ratings_pred = clf.predict(x_val)\n",
    "accuracy = accuracy_score(y_val, ratings_pred)\n",
    "print('Validation Accuracy: %.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42d81732-c01f-4342-9edb-f88c4082e6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 77.45%\n"
     ]
    }
   ],
   "source": [
    "# Fit best model on whole dataset and predict test data\n",
    "x_train = copy.deepcopy(training_test[use_cols].to_numpy())\n",
    "\n",
    "y_train = copy.deepcopy(training_test[['rating']])\n",
    "y_train = np.ravel(y_train)\n",
    "y_train = pd.cut(y_train, bins=[0,3,5], labels=[0,1])\n",
    "\n",
    "x_test = copy.deepcopy(test_removed[use_cols].to_numpy())\n",
    "y_test = copy.deepcopy(test_removed['rating']) \n",
    "\n",
    "y_test = np.ravel(y_test)\n",
    "y_test = pd.cut(y_test, bins=[0,3,5], labels=[0,1]) # convert continuous to categorical\n",
    "\n",
    "clf = XGBClassifier(eval_metric = 'logloss', eta=0.1, max_depth=5, min_child_weight = 4, n_estimators = 200, subsample = 0.70, n_jobs=-1, random_state=1, use_label_encoder = False)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "ratings_pred = clf.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, ratings_pred)\n",
    "print('Test Accuracy: %.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfd9b3d-8ce9-4cee-818a-880c85fa8fd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Regression Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "385cb6ac-b1ac-48e0-a780-18d5782a8db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.9038550284257149\n"
     ]
    }
   ],
   "source": [
    "# Fit best model on whole dataset and predict validation data\n",
    "use_cols = ['userId', 'movieId', 'userAvg', 'movieAvg', 'timestamp']\n",
    "use_cols.extend(genres)\n",
    "x_train = copy.deepcopy(training_validation[use_cols].to_numpy())\n",
    "\n",
    "y_train = copy.deepcopy(training_validation[['rating']])\n",
    "y_train = np.ravel(y_train)\n",
    "\n",
    "x_val = copy.deepcopy(validation_removed[use_cols].to_numpy())\n",
    "y_val = copy.deepcopy(validation_removed['rating']) \n",
    "y_val = np.ravel(y_val)\n",
    "\n",
    "clf = XGBRegressor(eval_metric = 'logloss', eta=0.1, max_depth=5, min_child_weight = 4, n_estimators = 200, subsample = 0.70, n_jobs=-1, random_state=1, use_label_encoder = False)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "ratings_pred = clf.predict(x_val)\n",
    "rmse = mean_squared_error(y_val, ratings_pred, squared = False)\n",
    "print('Validation RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07aebc65-5594-4c56-b6ea-f8f0ba86662a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.8682323577760647\n"
     ]
    }
   ],
   "source": [
    "# Fit best model on whole dataset and predict validation data\n",
    "x_train = copy.deepcopy(training_test[use_cols].to_numpy())\n",
    "y_train = copy.deepcopy(training_test[['rating']])\n",
    "y_train = np.ravel(y_train)\n",
    "\n",
    "x_test = copy.deepcopy(test_removed[use_cols].to_numpy())\n",
    "y_test = copy.deepcopy(test_removed['rating']) \n",
    "y_test = np.ravel(y_test)\n",
    "\n",
    "clf = XGBRegressor(eval_metric = 'logloss', eta=0.1, max_depth=5, min_child_weight = 4, n_estimators = 200, subsample = 0.70, n_jobs=-1, random_state=1, use_label_encoder = False)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "ratings_pred = clf.predict(x_test)\n",
    "rmse = mean_squared_error(y_test, ratings_pred, squared = False)\n",
    "print('Test RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859aca0a",
   "metadata": {},
   "source": [
    "#### .632+ Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c69c6c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nscores = bootstrap_point632_score(gs, x_train, y_train, n_splits=50, \\n                                  method='.632+', random_seed=123) \\nacc = np.mean(scores)\\nprint('Accuracy: %.2f%%' % (100*acc))\\n\\n# 95% confidence interval\\nlower = np.percentile(scores,2.5)*100\\nupper = np.percentile(scores,97.5)*100\\nprint('95%% Confidence interval: [%.2f, %.2f]' % (lower, upper))\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "scores = bootstrap_point632_score(gs, x_train, y_train, n_splits=50, \n",
    "                                  method='.632+', random_seed=123) \n",
    "acc = np.mean(scores)\n",
    "print('Accuracy: %.2f%%' % (100*acc))\n",
    "\n",
    "# 95% confidence interval\n",
    "lower = np.percentile(scores,2.5)*100\n",
    "upper = np.percentile(scores,97.5)*100\n",
    "print('95%% Confidence interval: [%.2f, %.2f]' % (lower, upper))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e462baef",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77f14113",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22404/996543886.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m123\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbest_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\JupyterLab\\resources\\jlab_server\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    935\u001b[0m         \"\"\"\n\u001b[0;32m    936\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 937\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m    938\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\JupyterLab\\resources\\jlab_server\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_classification\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\JupyterLab\\resources\\jlab_server\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;34m\"multilabel-sequences\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     ]:\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "best_model = DecisionTreeClassifier(**gs.best_params_, random_state=123)\n",
    "best_model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a30a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plot_tree(best_model, \n",
    "          filled=True, \n",
    "          rounded=True,\n",
    "          class_names=['0', \n",
    "                       '1'],\n",
    "          feature_names=training_test[use_cols].columns) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556dc5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_estimator = gs.best_estimator_\n",
    "tree_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afebd08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_estimator.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac31d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_values = tree_estimator.feature_importances_\n",
    "feature_importances = pd.Series(feature_importance_values, index=training_test[use_cols].columns) # x train's column values\n",
    "feature_top20 = feature_importances.sort_values(ascending=False)[:20]\n",
    "\n",
    "plt.figure(figsize=[8, 6])\n",
    "plt.title('Feature Importances Top 20')\n",
    "sns.barplot(x=feature_top20, y=feature_top20.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0079a9",
   "metadata": {},
   "source": [
    "- movieAverage was the most important feature in the prediction model\n",
    "    - movie's quality/social standard in general affects individuals' rate \n",
    "- userAverage was the second > so each person's rating standard/rating tendancy affects actual rate\n",
    "    - someone could send out 0 for dislike and 3 or like but someone else could send 5 for all movies\n",
    "    \n",
    "- timestamp represents seconds that have elapsed since the Unix epoch 00:00:00 UTC on midnight UTC of Janurary 1, 1970\n",
    "    - https://en.wikipedia.org/wiki/Unix_time\n",
    "    - Thus when the movie was watched also affects the rate\n",
    "    \n",
    "- genres in the other hand doesn't significantly affect the rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b14d2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_decision_regions(np.asarray(copy.deepcopy(training_test[plot_cols])).astype('float32'), np.array(copy.deepcopy(y_train)), gs)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
